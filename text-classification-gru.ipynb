{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10091416,"sourceType":"datasetVersion","datasetId":6222747}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Required Libraries\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.isri import ISRIStemmer\nimport nltk\nimport numpy as np\nimport pandas as pd\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load Dataset\ndataset = pd.read_csv('/kaggle/input/datasetdlarabic/aljazeera_data.csv')  # Update path if necessary\ndataset = dataset.dropna()  # Remove any null values\n\n# Preprocessing Pipeline\nclass TextPreprocessor:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('arabic'))\n        self.stemmer = ISRIStemmer()  # Arabic-specific stemmer\n\n    def preprocess(self, text):\n        # Tokenization\n        tokens = word_tokenize(text)\n        # Remove stop words\n        tokens = [t for t in tokens if t not in self.stop_words]\n        # Stemming\n        tokens = [self.stemmer.stem(t) for t in tokens]\n        return tokens\n\npreprocessor = TextPreprocessor()\ndataset['Processed_Text'] = dataset['Text'].apply(preprocessor.preprocess)\n\n# Encoding Labels\nlabel_encoder = LabelEncoder()\ndataset['Encoded_Score'] = label_encoder.fit_transform(dataset['Score'])\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset['Processed_Text'], dataset['Encoded_Score'], test_size=0.2, random_state=42\n)\n\n# Dataset Class\nclass NLPDataset(Dataset):\n    def __init__(self, texts, labels, vocab=None):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab or self.build_vocab()\n\n    def build_vocab(self):\n        vocab = set(token for text in self.texts for token in text)\n        return {word: idx for idx, word in enumerate(vocab, start=1)}\n\n    def encode_text(self, text):\n        return [self.vocab[token] for token in text if token in self.vocab]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoded_text = self.encode_text(self.texts.iloc[idx])\n        label = self.labels.iloc[idx]\n        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n\ntrain_dataset = NLPDataset(X_train, y_train)\ntest_dataset = NLPDataset(X_test, y_test, vocab=train_dataset.vocab)\n\ndef collate_fn(batch):\n    texts, labels = zip(*batch)\n    max_len = max(len(text) for text in texts)\n    padded_texts = [torch.cat([text, torch.zeros(max_len - len(text))]) for text in texts]\n    return torch.stack(padded_texts), torch.tensor(labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# GRU Model\nclass BiGRUModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(BiGRUModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n\n    def forward(self, x):\n        x = self.embedding(x.long())\n        _, hidden = self.gru(x)\n        # Combine forward and backward hidden states\n        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n        output = self.fc(hidden)\n        return output\n\n# Model Parameters\nvocab_size = len(train_dataset.vocab) + 1\nembed_dim = 128\nhidden_dim = 256\noutput_dim = len(label_encoder.classes_)\n\n# Initialize the GRU Model\nmodel = BiGRUModel(vocab_size, embed_dim, hidden_dim, output_dim)\n\n# Check if CUDA is available and move the model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training Settings\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 10\n\n# Training Loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for texts, labels in train_loader:\n        # Move data to GPU\n        texts, labels = texts.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# Evaluation Function\ndef evaluate_model(model, data_loader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for texts, labels in data_loader:\n            # Move data to GPU\n            texts, labels = texts.to(device), labels.to(device)\n\n            outputs = model(texts)\n            predicted = torch.argmax(outputs, dim=1)\n            y_true.extend(labels.tolist())\n            y_pred.extend(predicted.tolist())\n    return y_true, y_pred\n\n\n# Calculate Metrics\ny_train_true, y_train_pred = evaluate_model(model, train_loader)\ny_test_true, y_test_pred = evaluate_model(model, test_loader)\n\n\n# Training Metrics\ntrain_mse = mean_squared_error(y_train_true, y_train_pred)\ntrain_mae = mean_absolute_error(y_train_true, y_train_pred)\ntrain_r2 = r2_score(y_train_true, y_train_pred)\ntrain_acc = accuracy_score(y_train_true, y_train_pred)\n\n# Testing Metrics\ntest_mse = mean_squared_error(y_test_true, y_test_pred)\ntest_mae = mean_absolute_error(y_test_true, y_test_pred)\ntest_r2 = r2_score(y_test_true, y_test_pred)\ntest_acc = accuracy_score(y_test_true, y_test_pred)\n\n# Print Metrics\nprint(\"Training Metrics:\")\nprint(f\"MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R2: {train_r2:.4f}, Accuracy: {train_acc:.4f}\")\nprint(\"Testing Metrics:\")\nprint(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}, Accuracy: {test_acc:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T00:19:28.831918Z","iopub.execute_input":"2024-12-04T00:19:28.832576Z","iopub.status.idle":"2024-12-04T00:20:06.592606Z","shell.execute_reply.started":"2024-12-04T00:19:28.832540Z","shell.execute_reply":"2024-12-04T00:20:06.591695Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nEpoch 1, Loss: 1.0261\nEpoch 2, Loss: 0.5891\nEpoch 3, Loss: 0.4420\nEpoch 4, Loss: 0.3942\nEpoch 5, Loss: 0.2912\nEpoch 6, Loss: 0.2121\nEpoch 7, Loss: 0.1533\nEpoch 8, Loss: 0.1043\nEpoch 9, Loss: 0.0789\nEpoch 10, Loss: 0.0622\nTraining Metrics:\nMSE: 0.0548, MAE: 0.0106, R2: 0.9978, Accuracy: 0.9973\nTesting Metrics:\nMSE: 5.3958, MAE: 0.5901, R2: 0.6454, Accuracy: 0.9046\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}