{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10091416,"sourceType":"datasetVersion","datasetId":6222747}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import ssl\nimport nltk\nimport numpy as np\nimport pandas as pd\nssl._create_default_https_context = ssl._create_unverified_context\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Load Dataset\ndataset = pd.read_csv('/kaggle/input/datasetdlarabic/aljazeera_data.csv')  # Update path if necessary\ndataset = dataset.dropna()  # Remove any null values\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:31:59.384120Z","iopub.execute_input":"2024-12-03T19:31:59.384493Z","iopub.status.idle":"2024-12-03T19:31:59.440935Z","shell.execute_reply.started":"2024-12-03T19:31:59.384466Z","shell.execute_reply":"2024-12-03T19:31:59.439751Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.isri import ISRIStemmer\n\n# Preprocessing Pipeline\nclass TextPreprocessor:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('arabic'))\n        self.stemmer = ISRIStemmer()  # Use ISRIStemmer for Arabic\n\n    def preprocess(self, text):\n        # Tokenization\n        tokens = word_tokenize(text)\n        # Remove stop words\n        tokens = [t for t in tokens if t not in self.stop_words]\n        # Stemming\n        tokens = [self.stemmer.stem(t) for t in tokens]\n        return tokens\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:02.710509Z","iopub.execute_input":"2024-12-03T19:32:02.710884Z","iopub.status.idle":"2024-12-03T19:32:02.718784Z","shell.execute_reply.started":"2024-12-03T19:32:02.710853Z","shell.execute_reply":"2024-12-03T19:32:02.717695Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"preprocessor = TextPreprocessor()\ndataset['Processed_Text'] = dataset['Text'].apply(preprocessor.preprocess)\n\n# Encoding Labels\nlabel_encoder = LabelEncoder()\ndataset['Encoded_Score'] = label_encoder.fit_transform(dataset['Score'])\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset['Processed_Text'], dataset['Encoded_Score'], test_size=0.2, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:03.098029Z","iopub.execute_input":"2024-12-03T19:32:03.098955Z","iopub.status.idle":"2024-12-03T19:32:03.872742Z","shell.execute_reply.started":"2024-12-03T19:32:03.098923Z","shell.execute_reply":"2024-12-03T19:32:03.871886Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Dataset Class with Padding\nclass NLPDataset(Dataset):\n    def __init__(self, texts, labels, vocab=None, max_len=50):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab or self.build_vocab()\n        self.max_len = max_len\n\n    def build_vocab(self):\n        vocab = set(token for text in self.texts for token in text)\n        return {word: idx for idx, word in enumerate(vocab, start=1)}\n\n    def encode_text(self, text):\n        # Encode text and pad to `max_len`\n        encoded = [self.vocab[token] for token in text if token in self.vocab]\n        if len(encoded) < self.max_len:\n            encoded += [0] * (self.max_len - len(encoded))  # Pad with 0s\n        else:\n            encoded = encoded[:self.max_len]  # Truncate to max_len\n        return encoded\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoded_text = self.encode_text(self.texts.iloc[idx])\n        label = self.labels.iloc[idx]\n        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:03.874367Z","iopub.execute_input":"2024-12-03T19:32:03.874702Z","iopub.status.idle":"2024-12-03T19:32:03.883358Z","shell.execute_reply.started":"2024-12-03T19:32:03.874671Z","shell.execute_reply":"2024-12-03T19:32:03.882330Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_dataset = NLPDataset(X_train, y_train)\ntest_dataset = NLPDataset(X_test, y_test, vocab=train_dataset.vocab)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:03.982956Z","iopub.execute_input":"2024-12-03T19:32:03.983350Z","iopub.status.idle":"2024-12-03T19:32:03.999358Z","shell.execute_reply.started":"2024-12-03T19:32:03.983319Z","shell.execute_reply":"2024-12-03T19:32:03.998394Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        _, hidden = self.rnn(x)\n        output = self.fc(hidden.squeeze(0))\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:04.386159Z","iopub.execute_input":"2024-12-03T19:32:04.386812Z","iopub.status.idle":"2024-12-03T19:32:04.392806Z","shell.execute_reply.started":"2024-12-03T19:32:04.386778Z","shell.execute_reply":"2024-12-03T19:32:04.391758Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Model Parameters\nvocab_size = len(train_dataset.vocab) + 1\nembed_dim = 128\nhidden_dim = 256\noutput_dim = len(label_encoder.classes_)\n\nmodel = RNNModel(vocab_size, embed_dim, hidden_dim, output_dim)\n\n# Training Settings\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:38.137246Z","iopub.execute_input":"2024-12-03T19:32:38.137627Z","iopub.status.idle":"2024-12-03T19:32:38.149506Z","shell.execute_reply.started":"2024-12-03T19:32:38.137594Z","shell.execute_reply":"2024-12-03T19:32:38.148600Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Training Loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for texts, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:32:38.607481Z","iopub.execute_input":"2024-12-03T19:32:38.607861Z","iopub.status.idle":"2024-12-03T19:33:01.402156Z","shell.execute_reply.started":"2024-12-03T19:32:38.607830Z","shell.execute_reply":"2024-12-03T19:33:01.401011Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.3796\nEpoch 2, Loss: 1.0687\nEpoch 3, Loss: 1.1039\nEpoch 4, Loss: 1.1509\nEpoch 5, Loss: 1.0052\nEpoch 6, Loss: 0.9054\nEpoch 7, Loss: 0.8388\nEpoch 8, Loss: 0.8316\nEpoch 9, Loss: 0.8853\nEpoch 10, Loss: 0.9201\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Evaluation Metrics Function\ndef evaluate_model(model, data_loader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for texts, labels in data_loader:\n            outputs = model(texts)\n            predicted = torch.argmax(outputs, dim=1)\n            y_true.extend(labels.tolist())\n            y_pred.extend(predicted.tolist())\n    return y_true, y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:33:10.742245Z","iopub.execute_input":"2024-12-03T19:33:10.742894Z","iopub.status.idle":"2024-12-03T19:33:10.748455Z","shell.execute_reply.started":"2024-12-03T19:33:10.742858Z","shell.execute_reply":"2024-12-03T19:33:10.747217Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Calculate Metrics\ny_train_true, y_train_pred = evaluate_model(model, train_loader)\ny_test_true, y_test_pred = evaluate_model(model, test_loader)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:33:13.456405Z","iopub.execute_input":"2024-12-03T19:33:13.456817Z","iopub.status.idle":"2024-12-03T19:33:13.861278Z","shell.execute_reply.started":"2024-12-03T19:33:13.456784Z","shell.execute_reply":"2024-12-03T19:33:13.860373Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Training Metrics\ntrain_mse = mean_squared_error(y_train_true, y_train_pred)\ntrain_mae = mean_absolute_error(y_train_true, y_train_pred)\ntrain_r2 = r2_score(y_train_true, y_train_pred)\ntrain_acc = accuracy_score(y_train_true, y_train_pred)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:33:15.811099Z","iopub.execute_input":"2024-12-03T19:33:15.812087Z","iopub.status.idle":"2024-12-03T19:33:15.822152Z","shell.execute_reply.started":"2024-12-03T19:33:15.812045Z","shell.execute_reply":"2024-12-03T19:33:15.821104Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Testing Metrics\ntest_mse = mean_squared_error(y_test_true, y_test_pred)\ntest_mae = mean_absolute_error(y_test_true, y_test_pred)\ntest_r2 = r2_score(y_test_true, y_test_pred)\ntest_acc = accuracy_score(y_test_true, y_test_pred)\n\n# Print Metrics\nprint(\"Training Metrics:\")\nprint(f\"MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R2: {train_r2:.4f}, Accuracy: {train_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:33:16.917573Z","iopub.execute_input":"2024-12-03T19:33:16.917991Z","iopub.status.idle":"2024-12-03T19:33:16.927046Z","shell.execute_reply.started":"2024-12-03T19:33:16.917958Z","shell.execute_reply":"2024-12-03T19:33:16.925904Z"}},"outputs":[{"name":"stdout","text":"Training Metrics:\nMSE: 37.9629, MAE: 2.0371, R2: -0.5011, Accuracy: 0.8594\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"print(\"Testing Metrics:\")\nprint(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}, Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T19:33:22.816298Z","iopub.execute_input":"2024-12-03T19:33:22.816688Z","iopub.status.idle":"2024-12-03T19:33:22.822290Z","shell.execute_reply.started":"2024-12-03T19:33:22.816641Z","shell.execute_reply":"2024-12-03T19:33:22.821315Z"}},"outputs":[{"name":"stdout","text":"Testing Metrics:\nMSE: 26.2898, MAE: 1.5194, R2: -0.7279, Accuracy: 0.8799\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}