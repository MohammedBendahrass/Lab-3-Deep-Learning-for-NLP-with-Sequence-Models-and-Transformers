{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10091416,"sourceType":"datasetVersion","datasetId":6222747}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.translate.bleu_score import sentence_bleu\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import ISRIStemmer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load Dataset\ndataset = pd.read_csv('/kaggle/input/datasetdlarabic/aljazeera_data.csv')  # Update path if necessary\ndataset = dataset.dropna()  # Remove any null values\n\nclass TextPreprocessor:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('arabic'))\n        self.stemmer = ISRIStemmer()  # Arabic-specific stemmer\n\n    def preprocess(self, text):\n        # Tokenization\n        tokens = word_tokenize(text)\n        # Remove stop words\n        tokens = [t for t in tokens if t not in self.stop_words]\n        # Stemming\n        tokens = [self.stemmer.stem(t) for t in tokens]\n        return tokens\n\npreprocessor = TextPreprocessor()\ndataset['Processed_Text'] = dataset['Text'].apply(preprocessor.preprocess)\n\n# Encoding Labels\nlabel_encoder = LabelEncoder()\ndataset['Encoded_Score'] = label_encoder.fit_transform(dataset['Score'])\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset['Processed_Text'], dataset['Encoded_Score'], test_size=0.2, random_state=42\n)\n\n# Dataset Class\nclass NLPDataset(Dataset):\n    def __init__(self, texts, labels, vocab=None):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab or self.build_vocab()\n\n    def build_vocab(self):\n        vocab = set(token for text in self.texts for token in text)\n        return {word: idx for idx, word in enumerate(vocab, start=1)}\n\n    def encode_text(self, text):\n        return [self.vocab[token] for token in text if token in self.vocab]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoded_text = self.encode_text(self.texts.iloc[idx])\n        label = self.labels.iloc[idx]\n        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n\ntrain_dataset = NLPDataset(X_train, y_train)\ntest_dataset = NLPDataset(X_test, y_test, vocab=train_dataset.vocab)\n\ndef collate_fn(batch):\n    texts, labels = zip(*batch)\n    max_len = max(len(text) for text in texts)\n    padded_texts = [torch.cat([text, torch.zeros(max_len - len(text))]) for text in texts]\n    return torch.stack(padded_texts), torch.tensor(labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# Bidirectional RNN Model\nclass BiRNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(BiRNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n\n    def forward(self, x):\n        x = self.embedding(x.long())\n        _, hidden = self.rnn(x)\n        # Combine forward and backward hidden states\n        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n        output = self.fc(hidden)\n        return output\n\n# Model Parameters\nvocab_size = len(train_dataset.vocab) + 1\nembed_dim = 128\nhidden_dim = 256\noutput_dim = len(label_encoder.classes_)\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Instantiate model and move it to device (GPU if available)\nmodel = BiRNNModel(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\n\n# Training Settings\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 10\n\n# Training Loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for texts, labels in train_loader:\n        # Move data to the same device as the model (GPU if available)\n        texts, labels = texts.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# Evaluation Function including BLEU score\ndef evaluate_model(model, data_loader):\n    model.eval()\n    y_true, y_pred = [], []\n    bleu_scores = []\n    \n    with torch.no_grad():\n        for texts, labels in data_loader:\n            # Move data to device (GPU if available)\n            texts, labels = texts.to(device), labels.to(device)\n            outputs = model(texts)\n            predicted = torch.argmax(outputs, dim=1)\n            y_true.extend(labels.tolist())\n            y_pred.extend(predicted.tolist())\n            \n            # Calculate BLEU score\n            for true, pred in zip(labels, predicted):\n                reference = [str(true.item()).split()]  # Convert true label to string and tokenize\n                hypothesis = str(pred.item()).split()  # Convert predicted label to string and tokenize\n                bleu_score = sentence_bleu(reference, hypothesis)\n                bleu_scores.append(bleu_score)\n    \n    avg_bleu = np.mean(bleu_scores)\n    return y_true, y_pred, avg_bleu\n\n# Calculate Metrics including BLEU\ny_train_true, y_train_pred, train_bleu = evaluate_model(model, train_loader)\ny_test_true, y_test_pred, test_bleu = evaluate_model(model, test_loader)\n\n# Training Metrics\ntrain_mse = mean_squared_error(y_train_true, y_train_pred)\ntrain_mae = mean_absolute_error(y_train_true, y_train_pred)\ntrain_r2 = r2_score(y_train_true, y_train_pred)\ntrain_acc = accuracy_score(y_train_true, y_train_pred)\n\n# Testing Metrics\ntest_mse = mean_squared_error(y_test_true, y_test_pred)\ntest_mae = mean_absolute_error(y_test_true, y_test_pred)\ntest_r2 = r2_score(y_test_true, y_test_pred)\ntest_acc = accuracy_score(y_test_true, y_test_pred)\n\n# Print Metrics\nprint(\"Training Metrics:\")\nprint(f\"MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R2: {train_r2:.4f}, Accuracy: {train_acc:.4f}, BLEU: {train_bleu:.4f}\")\nprint(\"Testing Metrics:\")\nprint(f\"MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}, Accuracy: {test_acc:.4f}, BLEU: {test_bleu:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T00:30:28.666915Z","iopub.execute_input":"2024-12-04T00:30:28.667280Z","iopub.status.idle":"2024-12-04T00:30:40.316741Z","shell.execute_reply.started":"2024-12-04T00:30:28.667247Z","shell.execute_reply":"2024-12-04T00:30:40.315713Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nEpoch 1, Loss: 0.9742\nEpoch 2, Loss: 0.5513\nEpoch 3, Loss: 0.4051\nEpoch 4, Loss: 0.3053\nEpoch 5, Loss: 0.2170\nEpoch 6, Loss: 0.1668\nEpoch 7, Loss: 0.1079\nEpoch 8, Loss: 0.0803\nEpoch 9, Loss: 0.0510\nEpoch 10, Loss: 0.0362\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Training Metrics:\nMSE: 0.5137, MAE: 0.0309, R2: 0.9797, Accuracy: 0.9973, BLEU: 0.9973\nTesting Metrics:\nMSE: 6.5230, MAE: 0.6926, R2: 0.5713, Accuracy: 0.9011, BLEU: 0.9011\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}